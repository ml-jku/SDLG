{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from utils import compute_semantic_clusters, compute_semantic_entropy, prepare_results\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['coqa', 'trivia_qa', 'truthful_qa'][0]\n",
    "correctness_metric = [\"rougeL\", \"bleurt\", \"rouge1\"][2]\n",
    "\n",
    "if dataset == 'truthful_qa':\n",
    "    correctness_metric += \"-diff\"\n",
    "\n",
    "correctness_threshold_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "num_total_gens = 10\n",
    "\n",
    "run_ids = [\"coqa_opt-2.7b\"]\n",
    "\n",
    "model_type = \"deberta-large-mnli\"\n",
    "auroc_keys = [\"normalised_semantic_entropy\", \"unnormalised_semantic_entropy\"] \n",
    "# auroc_keys += [\"cleaned_normalised_semantic_entropy\", \"cleaned_unnormalised_semantic_entropy\"]\n",
    "\n",
    "log_results = True\n",
    "plot_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cleaned = \"cleaned_normalised_semantic_entropy\" in auroc_keys or \"cleaned_unnormalised_semantic_entropy\" in auroc_keys\n",
    "\n",
    "\n",
    "\n",
    "if plot_results:\n",
    "    fig_dict = {}\n",
    "    for correctness_threshold in correctness_threshold_list:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=len(auroc_keys), figsize=(5*len(auroc_keys), 5))\n",
    "        fig_dict[correctness_threshold] = {\"fig\": fig, \"axs\": axs}\n",
    "\n",
    "color_count = 0\n",
    "for run_id in run_ids:\n",
    "\n",
    "    basepath = os.path.join('results', run_id)\n",
    "\n",
    "    for run_key in [\"sdlg\", \"baseline\"]:\n",
    "\n",
    "        if plot_results:\n",
    "            plot_dict, plot_dict_kuhn = {}, {}\n",
    "            for correctness_threshold in correctness_threshold_list:\n",
    "                plot_dict[correctness_threshold] = {'plot_data_x': [], 'plot_data_y': []}\n",
    "                plot_dict_kuhn[correctness_threshold] = {'plot_data_x': [], 'plot_data_y': []}\n",
    "\n",
    "        list_results_dict, list_rougeL, dataset_size = prepare_results(num_samples=8000,\n",
    "                                                                        run_key=run_key,\n",
    "                                                                        metric=correctness_metric,\n",
    "                                                                        start_sample_id=0,\n",
    "                                                                        base_path=basepath)\n",
    "\n",
    "        # iterate over num_gens\n",
    "        for num_gens in tqdm(range(2, num_total_gens+1)):\n",
    "\n",
    "            all_semantic_entropies, all_semantic_entropies_kuhn = [], []\n",
    "            list_num_semantic_clusters, list_num_generations = [], []\n",
    "            \n",
    "            # iterate over instances\n",
    "            for i, results_dict in enumerate(list_results_dict):\n",
    "\n",
    "                # ---------- create mask of considered generations\n",
    "                if num_gens < len(results_dict[run_key]['generations']):\n",
    "                    boolean_mask = [True] * num_gens + [False] * (len(results_dict[run_key]['generations']) - num_gens)\n",
    "                else:\n",
    "                    boolean_mask = [True] * len(results_dict[run_key]['generations'])\n",
    "\n",
    "                boolean_mask = torch.tensor(boolean_mask)\n",
    "\n",
    "                if run_key == \"sdlg\":\n",
    "                    mask = torch.tensor([1] + [gen['token_likelihood'] for gen in results_dict[run_key]['generations'][1:]])\n",
    "                    assert torch.all(mask > 0) and torch.all(mask[1:] < 1), f\"mask: {mask}\"\n",
    "                elif run_key == \"baseline\":\n",
    "                    mask = boolean_mask\n",
    "                # ----------\n",
    "\n",
    "                list_num_generations.append(torch.sum(boolean_mask).item())\n",
    "\n",
    "                all_considered_generations, all_considered_likelihoods = [], []\n",
    "                for m, included in enumerate(boolean_mask):\n",
    "                    if included:\n",
    "                        all_considered_generations.append(results_dict[run_key]['generations'][m])\n",
    "                        all_considered_likelihoods.append(results_dict[run_key]['likelihoods'][m])\n",
    "\n",
    "                if results_dict[run_key][f'semantic_pairs_{model_type}']['semantic_pairs'].shape[0] > 1:\n",
    "                    semantic_pairs = results_dict[run_key][f'semantic_pairs_{model_type}']['semantic_pairs'][boolean_mask, :][:, boolean_mask]\n",
    "                    if compute_cleaned:\n",
    "                        cleaned_semantic_pairs = results_dict[run_key][f'semantic_pairs_{model_type}']['cleaned_semantic_pairs'][boolean_mask, :][:, boolean_mask]\n",
    "                else:\n",
    "                    # deal with single generations\n",
    "                    assert boolean_mask.item() == True, f\"mask: {boolean_mask}\"\n",
    "                    semantic_pairs = results_dict[run_key][f'semantic_pairs_{model_type}']['semantic_pairs']\n",
    "                    if compute_cleaned:\n",
    "                        cleaned_semantic_pairs = results_dict[run_key][f'semantic_pairs_{model_type}']['cleaned_semantic_pairs']\n",
    "\n",
    "                # compute symmetric adjacency matrix\n",
    "                semantic_pairs = semantic_pairs & semantic_pairs.T\n",
    "                assert np.array_equal(semantic_pairs, semantic_pairs.T)\n",
    "                if compute_cleaned:\n",
    "                    cleaned_semantic_pairs = cleaned_semantic_pairs & cleaned_semantic_pairs.T\n",
    "                    assert np.array_equal(cleaned_semantic_pairs, cleaned_semantic_pairs.T)\n",
    "\n",
    "                # compute semantic clusters\n",
    "                semantic_difference = compute_semantic_clusters(generations=all_considered_generations, \n",
    "                                                                        cleaned_semantic_pairs=cleaned_semantic_pairs if compute_cleaned else None,\n",
    "                                                                        semantic_pairs=semantic_pairs,\n",
    "                                                                        compute_cleaned=compute_cleaned)\n",
    "                list_num_semantic_clusters.append(torch.unique(semantic_difference[\"semantic_clusters\"]).shape[0])\n",
    "\n",
    "                # compute semantic entropy\n",
    "                weights = boolean_mask[boolean_mask].to(torch.float32) if run_key == \"baseline\" else torch.nn.functional.normalize(mask[boolean_mask].to(torch.float32), p=1, dim=0)\n",
    "                all_semantic_entropies.append(\n",
    "                    compute_semantic_entropy(weights=weights,\n",
    "                                             mc_estimate_over_clusters=False,\n",
    "                                             neg_log_likelihoods=all_considered_likelihoods, \n",
    "                                             semantic_difference=semantic_difference,\n",
    "                                             compute_cleaned=compute_cleaned))\n",
    "\n",
    "                if run_key == \"baseline\":\n",
    "                    all_semantic_entropies_kuhn.append(\n",
    "                        compute_semantic_entropy(weights=weights,\n",
    "                                                 mc_estimate_over_clusters=True,\n",
    "                                                 neg_log_likelihoods=all_considered_likelihoods, \n",
    "                                                 semantic_difference=semantic_difference,\n",
    "                                                 compute_cleaned=compute_cleaned))\n",
    "\n",
    "            for r, correctness_threshold in enumerate(correctness_threshold_list):\n",
    "                list_correct_labels = torch.logical_not((torch.tensor(list_rougeL) >= correctness_threshold))\n",
    "                aurocs, aurocs_kuhn = {}, {}\n",
    "                for k, key in enumerate(auroc_keys):\n",
    "                    aurocs[key] = roc_auc_score(list_correct_labels,\n",
    "                                                [d[key] for d in all_semantic_entropies])\n",
    "\n",
    "                    if run_key == \"baseline\":\n",
    "                        aurocs_kuhn[key] = roc_auc_score(list_correct_labels,\n",
    "                                                        [d[key] for d in all_semantic_entropies_kuhn])\n",
    "\n",
    "                if plot_results:\n",
    "                    plot_dict[correctness_threshold]['plot_data_x'].append(num_gens) # sum(list_num_generations) / len(list_num_generations))\n",
    "                    plot_dict[correctness_threshold]['plot_data_y'].append(aurocs)\n",
    "\n",
    "                    if run_key == \"baseline\":\n",
    "                        plot_dict_kuhn[correctness_threshold]['plot_data_x'].append(num_gens)\n",
    "                        plot_dict_kuhn[correctness_threshold]['plot_data_y'].append(aurocs_kuhn)\n",
    "                    \n",
    "                if log_results:\n",
    "                    with open(os.path.join(basepath, f'auroc_results.csv'), 'a') as f:\n",
    "                        writer = csv.writer(f)\n",
    "                        writer.writerow([run_id, \n",
    "                                         run_key, \n",
    "                                         num_gens, \n",
    "                                         correctness_threshold, \n",
    "                                         \"importance_sampling\", \n",
    "                                         aurocs[\"normalised_semantic_entropy\"], \n",
    "                                         aurocs[\"unnormalised_semantic_entropy\"]])\n",
    "                        if run_key == \"baseline\":\n",
    "                            writer.writerow([run_id, \n",
    "                                             run_key, \n",
    "                                             num_gens, \n",
    "                                             correctness_threshold, \n",
    "                                             \"mc_estimate_over_clusters\", \n",
    "                                             aurocs_kuhn[\"normalised_semantic_entropy\"], \n",
    "                                             aurocs_kuhn[\"unnormalised_semantic_entropy\"]])\n",
    "\n",
    "                    with open(os.path.join(basepath, f'avg_num_clusters_found.csv'), 'a') as f:\n",
    "                        writer = csv.writer(f)\n",
    "                        clusters_correct_answer = torch.tensor(list_num_semantic_clusters)[list_correct_labels == 0]\n",
    "                        clusters_wrong_answer = torch.tensor(list_num_semantic_clusters)[list_correct_labels == 1]\n",
    "                        writer.writerow([run_id, run_key, num_gens, correctness_threshold, \n",
    "                                        sum(list_num_semantic_clusters) / len(list_num_semantic_clusters), \n",
    "                                        torch.mean(clusters_correct_answer.float()).item(), sum(list_correct_labels).item(),\n",
    "                                        torch.mean(clusters_wrong_answer.float()).item(), (len(list_correct_labels) - sum(list_correct_labels)).item()])\n",
    "\n",
    "        if plot_results:\n",
    "            for correctness_threshold in correctness_threshold_list:\n",
    "                for k, key in enumerate(auroc_keys):\n",
    "                    x_values = plot_dict[correctness_threshold]['plot_data_x']\n",
    "                    y_values = [d[key] for d in plot_dict[correctness_threshold]['plot_data_y']]\n",
    "                    fig_dict[correctness_threshold][\"axs\"][k].plot(x_values, \n",
    "                                                                   y_values, \n",
    "                                                                   c=f\"C{color_count}\", \n",
    "                                                                   alpha=0.8, \n",
    "                                                                   marker=\"o\" if run_key==\"sdlg\" else \"^\", \n",
    "                                                                   label=f\"{run_id} ({run_key})\")\n",
    "                    \n",
    "                    if run_key == \"baseline\":\n",
    "                        x_values = plot_dict_kuhn[correctness_threshold]['plot_data_x']\n",
    "                        y_values = [d[key] for d in plot_dict_kuhn[correctness_threshold]['plot_data_y']]\n",
    "                        fig_dict[correctness_threshold][\"axs\"][k].plot(x_values, \n",
    "                                                                       y_values, \n",
    "                                                                       c=f\"C{color_count}\", \n",
    "                                                                       alpha=0.4, \n",
    "                                                                       marker=\"v\", \n",
    "                                                                       label=f\"{run_id} (incorrect estimate)\")\n",
    "\n",
    "                    fig_dict[correctness_threshold][\"axs\"][k].set_xlabel('# generations')\n",
    "                    fig_dict[correctness_threshold][\"axs\"][k].set_ylabel(f'AUROC')\n",
    "                    fig_dict[correctness_threshold][\"axs\"][k].set_xlim([1, num_total_gens+1])\n",
    "                    fig_dict[correctness_threshold][\"axs\"][k].set_xticks(range(2, num_total_gens+1, 2))\n",
    "                    fig_dict[correctness_threshold][\"axs\"][k].set_ylim([0.5, 1])\n",
    "                    fig_dict[correctness_threshold][\"axs\"][k].legend()\n",
    "                    fig_dict[correctness_threshold][\"axs\"][k].set_title(key)\n",
    "                    \n",
    "                    if run_key == \"baseline\":\n",
    "                        fig_dict[correctness_threshold][\"fig\"].suptitle(f'Correctness threshold: {correctness_threshold}', fontsize=12)\n",
    "                        fig_dict[correctness_threshold][\"fig\"].tight_layout()\n",
    "                        fig_dict[correctness_threshold][\"fig\"].savefig(os.path.join(basepath, f'{model_type}_{dataset_size}samples_{correctness_threshold}threshold.png'))\n",
    "\n",
    "        color_count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qualm_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
